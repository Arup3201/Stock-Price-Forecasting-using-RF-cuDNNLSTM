{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QzwTHP_99jFM"
      },
      "outputs": [],
      "source": [
        "# DEMO CODEðŸ‘‡\n",
        "%%time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mA6hcrd0Vn0s"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_mXte9KV9xD"
      },
      "source": [
        "RAPIDSAI referenec: https://docs.rapids.ai/install#rapids-release-selector"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --target=$nb_path cuml_cu11==23.04.00 --extra-index-url https://pypi.nvidia.com \n",
        "!pip install --target=$nb_path cudf_cu11==23.04.00 --extra-index-url https://pypi.nvidia.com "
      ],
      "metadata": {
        "id": "f2gRyr-djsZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rg1yPnfL2EAP"
      },
      "outputs": [],
      "source": [
        "# For data processing\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# For Visulization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# For GPU Processing\n",
        "import cudf\n",
        "import cupy as cp\n",
        "import cuml\n",
        "\n",
        "# Save features as a pickle file\n",
        "import pickle\n",
        "\n",
        "# For Random Forest using GPU\n",
        "from cuml.ensemble import RandomForestRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yXUCYnyexYSo"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/datasets/stock-prices-S&P-constituents/datasets-list', 'rb') as file:\n",
        "    datasets = pickle.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/datasets/stock-prices-S&P-constituents/symbols', 'rb') as file:\n",
        "  symbols = pickle.load(file)"
      ],
      "metadata": {
        "id": "wnbCGGh62yU9"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTs7YIN5xgsl",
        "outputId": "614c9651-dec7-4311-c0c2-b3a5c7e32b66"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "26"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "len(datasets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNpfuL4LxtXs"
      },
      "outputs": [],
      "source": [
        "# This function will generate new features for Random Forest\n",
        "def generate_features_rf(current_data):\n",
        "    curr_dataset = cp.asarray(current_data.copy())\n",
        "\n",
        "    # Take the total amount of days in 1st study period\n",
        "    T_study = curr_dataset.shape[1]\n",
        "\n",
        "    # Create the t =[241, 243, ..., T_study]\n",
        "    t = cp.arange(240, T_study)\n",
        "\n",
        "    # Define the m for calculation of t-m, m = [1, 2, 3, ..., 20]\n",
        "    M = cp.arange(1, 21)\n",
        "\n",
        "    # m = [1, 2, 3, ..., 20] U [40, 60, 80, ..., 240]\n",
        "    M = cp.concatenate((M, cp.arange(40, 241, 20)))\n",
        "\n",
        "    # Define number of stocks as it will be used to create arrays with proper shapes\n",
        "    n_stocks = 251\n",
        "\n",
        "    # Create a container to store ir, cr and or for the current dataset and targets to store targets\n",
        "    features_container = cp.ones(shape=(n_stocks, T_study, M.shape[0]*3))\n",
        "    targets = cp.ones(shape=(n_stocks, T_study, 1))\n",
        "\n",
        "    # Put NaN values to the first 240 rows as it will be used for feature creation\n",
        "    features_container[:, :t[0], :] = cp.nan \n",
        "    targets[:, :t[0], :] = cp.nan\n",
        "\n",
        "    # To calculate ir, we need cp_(t-m) and op_(t-m)\n",
        "    cp_t_m = cp.zeros((n_stocks, t.shape[0], M.shape[0]))\n",
        "    op_t_m = cp.zeros((n_stocks, t.shape[0], M.shape[0]))\n",
        "\n",
        "    # To calculate cr, we need cp_(t-1-m) and cp_(t-1-m). Remember we are indexing from 0, not 1!\n",
        "    cp_t_1_m = cp.zeros((n_stocks, t.shape[0], M.shape[0]))\n",
        "    cp_t_1 = curr_dataset[:, t-2, 0]\n",
        "\n",
        "    # To calculate or, we need op_t and cp_t_m. Remember we are indexing from 0, not 1!\n",
        "    op_t = curr_dataset[:, t-1, 1]\n",
        "\n",
        "\n",
        "    # Calculate cp_(t-m), op_(t-m) and cp_(t-1-m) for each m and store them at proper axis=2 index i\n",
        "    # of their respective container\n",
        "    for i, m in enumerate(M):\n",
        "        cp_t_m[:, :, i] = curr_dataset[:, t-m, 0]\n",
        "        op_t_m[:, :, i] = curr_dataset[:, t-m, 1]\n",
        "        cp_t_1_m[:, :, i] = curr_dataset[:, t-1-m, 0]\n",
        "\n",
        "\n",
        "    # Calculate ir_(t-m)\n",
        "    ir_t_m = cp.divide(cp_t_m, op_t_m, out=cp.zeros_like(cp_t_m)) - 1\n",
        "\n",
        "\n",
        "    # Before calculating cr_(t-m), reshape the cp_(t-1-m) as it should have the same last part of shape as cp_(t-1), the divident\n",
        "    # means if cp_(t-1) is (251, 774) then cp_(t-1-m) should be (_, 251, 774) notice the last of shape is same\n",
        "    reshaped_cp_t_1_m = cp_t_1_m.reshape(M.shape[0], n_stocks, -1)\n",
        "\n",
        "    # Calculating cr_(t-m)\n",
        "    cr_t_m = cp.divide(cp_t_1, reshaped_cp_t_1_m).reshape(n_stocks, -1, M.shape[0]) - 1\n",
        "\n",
        "\n",
        "    # Before calculating or_(t-m), reshape the cp_(t-m) as it should have the same last part of shape as op_t, the divident\n",
        "    # means if op_t is (251, 774) then cp_(t-m) should be (_, 251, 774) notice the last of shape is same\n",
        "    reshaped_cp_t_m = cp_t_m.reshape(M.shape[0], n_stocks, -1)\n",
        "\n",
        "    # Calculating or_(t-m)\n",
        "    or_t_m = cp.divide(op_t, reshaped_cp_t_m).reshape(n_stocks, -1, M.shape[0]) - 1\n",
        "\n",
        "    # Calculate targets\n",
        "    cp_t0 = curr_dataset[:, t, 0:1] # 0:1 makes sure that it is 3D just like targets[:, t, :]\n",
        "    op_t0 = curr_dataset[:, t, 1:] # 1: makes sure that it is 3D just like targets[:, t, :]\n",
        "    targets[:, t, :] = cp.divide(cp_t0, op_t0, out=cp.zeros_like(cp_t0), _where=op_t0!=0)\n",
        "\n",
        "    # Put the ir, cr and or inside the container\n",
        "    features_container[:, t, :] = cp.dstack((ir_t_m, cr_t_m, or_t_m))\n",
        "\n",
        "    return features_container, targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGp9P1r5xzXF"
      },
      "outputs": [],
      "source": [
        "# It will contain all the newly processed datasets each with a shape (251, stock days in 4 years, 93)\n",
        "containers = []\n",
        "targets = []\n",
        "\n",
        "# Run the generate_feature_rf function for each dataset inside main_datasets\n",
        "for dataset in datasets:\n",
        "  dataset = dataset.astype(np.float64)\n",
        "  features, target = generate_features_rf(dataset)\n",
        "  containers.append(features)\n",
        "  targets.append(target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncEUobXUx14o",
        "outputId": "f7ce7644-7298-4d98-f043-45da5bf906bd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(26, 26)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "len(containers), len(targets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwc7OhhI09f4"
      },
      "source": [
        "251 Stocks, ~1008 Days recorded, 93 Features"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rf_reg = RandomForestRegressor(n_estimators=1000, max_depth=10)\n",
        "\n",
        "train_size = 252 * 2\n",
        "test_size = 252\n",
        "\n",
        "for curr_dataset, curr_targets in zip(containers, targets):\n",
        "  X = curr_dataset[:, 252:, :]\n",
        "  y = curr_targets[:, 252:, :]\n",
        "\n",
        "  # Split the dataset into train and test data\n",
        "  X_train = X[:train_size]\n",
        "  X_test = X[train_size:]\n",
        "  y_train = y[:train_size]\n",
        "  y_test = y[train_size:]\n",
        "\n",
        "  # Initialize the names for each array dimension to refer them when reshaping\n",
        "  n_stocks = X_train.shape[0]\n",
        "  n_train_days = X_train.shape[1]\n",
        "  n_test_days = X_test.shape[1]\n",
        "  n_features = X_train.shape[2]\n",
        "\n",
        "  # Matching the shape for training\n",
        "  X_train = X_train.reshape(-1, 93)\n",
        "  y_train = y_train.reshape(-1, 1)\n",
        "\n",
        "  # Training\n",
        "  rf_reg.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "  # Prediction\n",
        "  y_pred = rf_reg.predict(X_train)\n",
        "\n",
        "  # Reverse back the shapes to map the stock indexes to symbol list\n",
        "  y_train = y_train.reshape(n_stocks, n_train_days, 1)\n",
        "  y_pred = y_pred.reshape(n_stocks, n_train_days, 1)\n",
        "\n",
        "  # No of stocks to fetch from top and bottom\n",
        "  k = 5\n",
        "\n",
        "  # Find the top and bottom k stocks using argsort which will return the indexes\n",
        "  top_k_stocks = cp.argsort(y_pred, axis=0)[:k,:,-1]\n",
        "  bottom_k_stocks = cp.argsort(y_pred, axis=0)[-k:,:,-1]\n",
        "\n",
        "  # Print the Predicted top k and bottom k stocks\n",
        "  print(\"Predicted: \")\n",
        "  for day in range(n_train_days):\n",
        "    print(f\"Day {day}:\")\n",
        "    # Iterate over all k stock indixces and as they are type cupy.array(), convert it to scalar using .item()\n",
        "    print(\"Top k: \", ' '.join([f\"{symbols[i.item()]}\" for i in top_k_stocks[:, day]]))\n",
        "    print(\"Bottom k: \", ' '.join([f\"{symbols[i.item()]}\" for i in bottom_k_stocks[:, day]]))\n",
        "\n",
        "  top_k_stocks = cp.argsort(y_train, axis=0)[:k,:,-1]\n",
        "  bottom_k_stocks = cp.argsort(y_train, axis=0)[-k:,:,-1]\n",
        "\n",
        "  # Print the Actual top k and bottom k stocks\n",
        "  print(\"Actual: \")\n",
        "  for day in range(n_train_days):\n",
        "    print(f\"Day {day}:\")\n",
        "    # Iterate over all k stock indixces and as they are type cupy.array(), convert it to scalar using .item()\n",
        "    print(\"Top k:\", ' '.join([f\"{symbols[i.item()]}\" for i in top_k_stocks[:, day]]))\n",
        "    print(\"Bottom k:\", ' '.join([f\"{symbols[i.item()]}\" for i in bottom_k_stocks[:, day]]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ABO8AK_ibB8",
        "outputId": "787114bf-98f3-4833-ad23-d95cd5b243b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(191011, 93) (191011, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vk8XCzyer22F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To train the model using 26 datasets where each dataset contains 4 years of stock data, we can follow a similar approach as before. However, we need to make some modifications to account for the fact that each dataset contains a 4-year window of data.\n",
        "\n",
        "Here's a modified code snippet to load the data and split it into training and testing sets for each dataset:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Load the data and split it into 26 datasets\n",
        "# Each dataset contains 4 years of stock data\n",
        "data = np.load('data.npy')\n",
        "n_datasets = 26\n",
        "dataset_size = data.shape[1] // n_datasets\n",
        "datasets = [data[:, i*dataset_size:(i+1)*dataset_size, :] for i in range(n_datasets)]\n",
        "\n",
        "# Split each dataset into training and testing sets\n",
        "train_size = 2 * 252  # 2 years of data for training\n",
        "test_size = 252  # 1 year of data for testing\n",
        "X_train_list, y_train_list, X_test_list, y_test_list = [], [], [], []\n",
        "for dataset in datasets:\n",
        "    X_train, y_train = [], []\n",
        "    X_test, y_test = [], []\n",
        "    for i in range(dataset_size - train_size - test_size):\n",
        "        # Use data from first year for feature creation\n",
        "        features = generate_features_rf(dataset[:, i:i+train_size+test_size, :])\n",
        "        X = features[0][252:,:]  # Use data from second year onwards for training/testing\n",
        "        y = features[1][252:,:]\n",
        "        X_train.append(X[:train_size, :])\n",
        "        y_train.append(y[:train_size, :])\n",
        "        X_test.append(X[train_size:, :])\n",
        "        y_test.append(y[train_size:, :])\n",
        "    X_train_list.append(np.array(X_train))\n",
        "    y_train_list.append(np.array(y_train))\n",
        "    X_test_list.append(np.array(X_test))\n",
        "    y_test_list.append(np.array(y_test))\n",
        "\n",
        "```\n",
        "\n",
        "Next, we can train the random forest model on all 26 datasets at once using the `multi_output=True` parameter in `RandomForestRegressor`:\n",
        "\n",
        "```python\n",
        "# Train a random forest model on all 26 datasets at once\n",
        "rf = RandomForestRegressor(n_estimators=100, n_jobs=-1, random_state=42, multi_output=True)\n",
        "X_train_all = np.concatenate(X_train_list, axis=0)\n",
        "y_train_all = np.concatenate(y_train_list, axis=0)\n",
        "rf.fit(X_train_all, y_train_all)\n",
        "```\n",
        "\n",
        "To make predictions for a given day `t`, we can use the same approach as before. However, we need to modify the feature creation process to only use the opening and closing prices up to day `t-1`, since we cannot use the stock prices for day `t` in feature creation:\n",
        "\n",
        "```python\n",
        "# Load the opening and closing prices up to day t-1 for all stocks\n",
        "prices = np.load('prices.npy')\n",
        "prices = prices[:, :t, :]\n",
        "\n",
        "# Use the opening and closing prices to generate features for day t\n",
        "features = generate_features_rf(prices)\n",
        "X = features[0][-1:, :]  # Use the last day's features for prediction\n",
        "y_pred = rf.predict(X)\n",
        "\n",
        "# Get the indices of the k stocks with highest and lowest predicted intraday return\n",
        "k = 10\n",
        "top_k = np.argsort(y_pred.flatten())[-k:]\n",
        "bottom_k = np.argsort(y_pred.flatten())[:k]\n",
        "```\n",
        "Here, `prices` is a numpy array of shape `(n_stocks, t, 2)`, where the last dimension corresponds to the opening and closing prices for each day up to day `t-1`. We use `generate_features`.\n",
        "\n",
        "**It was the last solution I got from ChatGPT.**"
      ],
      "metadata": {
        "id": "J9p6_HSliMO6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Other Solutions I got from chatGPT - \n",
        "\n",
        "One approach you can try is to concatenate all the 26 datasets into a single dataset and train a Random Forest Classifier on this larger dataset. This approach will likely be faster as you won't need to train 26 separate models. Additionally, this approach may allow the Random Forest Classifier to learn more complex patterns that exist across all the stocks in the S&P500 index, potentially leading to better accuracy.\n",
        "\n",
        "To implement this approach, you can concatenate all the feature and target arrays from the 26 datasets into a single feature and target array. The resulting feature array will have shape (26251508, 93), and the target array will have shape (26251508,). Here's an example code snippet:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# assume feature_arrays and target_arrays are lists of length 26 containing feature and target arrays, respectively\n",
        "concatenated_features = np.concatenate(feature_arrays, axis=0)\n",
        "concatenated_targets = np.concatenate(target_arrays, axis=0)\n",
        "```\n",
        "After concatenating the feature and target arrays, you can split them into training and testing sets as you were doing before. One approach you can use is to split the concatenated arrays into training and testing sets based on the number of samples in each dataset. Specifically, you can take the first 20 datasets (i.e., the first 20 years of data) for training and the remaining 6 datasets (i.e., the last 6 years of data) for testing. Here's an example code snippet:\n",
        "\n",
        "```python\n",
        "train_features = concatenated_features[:20*251*508]\n",
        "train_targets = concatenated_targets[:20*251*508]\n",
        "test_features = concatenated_features[20*251*508:]\n",
        "test_targets = concatenated_targets[20*251*508:]\n",
        "```\n",
        "\n",
        "Finally, you can train a Random Forest Classifier on the training set and evaluate its accuracy on the testing set. Here's an example code snippet:\n",
        "\n",
        "```python\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(train_features, train_targets)\n",
        "\n",
        "predictions = rf.predict(test_features)\n",
        "accuracy = accuracy_score(test_targets, predictions)\n",
        "print(\"Accuracy: \", accuracy)\n",
        "```\n",
        "You can experiment with different values of hyperparameters such as the number of trees in the forest (i.e., n_estimators) and the depth of the trees (i.e., max_depth) to optimize the accuracy of the Random Forest Classifier.\n",
        "\n",
        "\n",
        "As the previous answer was not proper for me, I asked,\n",
        "\n",
        "```\n",
        "It is a great solution. \n",
        "But, I forgot to mention the goal. \n",
        "The goal is to predict k stocks with highest and \n",
        "k stocks with lowest intraday return out of all n stocks. \n",
        "Actually, the target has shape (251, 1008, 1), \n",
        "It contains the intraday return value for each day (so, 1008) and for each stock (so, 251). \n",
        "But, if I do what you just explained then \n",
        "I will lose the way of keeping track of the stocks. \n",
        "If that happens I can't answer which stocks have highest intraday return and \n",
        "which has lowest. Keeping you previous answer in mind, \n",
        "can you give me a solution for this problem?\n",
        "```\n",
        "\n",
        "<i>Sure! In that case, you can modify the approach to train a separate Random Forest Classifier for each of the two prediction tasks: (1) predicting the k stocks with highest intraday return and (2) predicting the k stocks with lowest intraday return.<br>\n",
        "To do this, you can first calculate the average intraday return for each stock over the training period (i.e., from the 240th day to the 756th day) and sort the stocks in ascending order of their average intraday returns to get the k stocks with lowest average returns. Similarly, you can sort the stocks in descending order of their average intraday returns to get the k stocks with highest average returns. These k stocks will be the targets for the two prediction tasks.<br>\n",
        "Next, you can concatenate the feature arrays across the 26 datasets into a single feature array as described in the previous solution. However, you will need to reshape the target array to have shape (26*1008, 251) to preserve the mapping between the intraday return values and the corresponding stocks.<br>\n",
        "Then, for each of the two prediction tasks, you can extract the target values corresponding to the k stocks with highest/lowest average intraday returns and train a separate Random Forest Classifier on the concatenated feature array and the corresponding target values. You can use the trained classifiers to predict the intraday returns for the testing period (i.e., from the 757th day till the end) and rank the stocks based on their predicted intraday returns to get the k stocks with highest/lowest predicted intraday returns.</i>\n",
        "\n",
        "Here's an example code snippet to implement this approach:\n",
        "```python\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# assume feature_arrays and target_arrays are lists of length 26 containing feature and target arrays, respectively\n",
        "concatenated_features = np.concatenate(feature_arrays, axis=0)\n",
        "concatenated_targets = np.concatenate(target_arrays, axis=0).reshape(-1, 251)\n",
        "\n",
        "# calculate average intraday returns for each stock over the training period\n",
        "avg_returns = np.mean(concatenated_targets[:, 240:756], axis=1)\n",
        "\n",
        "# get indices of k stocks with highest/lowest average intraday returns\n",
        "k = 5\n",
        "highest_indices = np.argsort(-avg_returns)[:k]\n",
        "lowest_indices = np.argsort(avg_returns)[:k]\n",
        "\n",
        "# extract target values corresponding to k stocks with highest/lowest average intraday returns\n",
        "highest_targets = concatenated_targets[:, highest_indices]\n",
        "lowest_targets = concatenated_targets[:, lowest_indices]\n",
        "\n",
        "# train separate Random Forest Classifiers for the two prediction tasks\n",
        "rf_highest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_highest.fit(concatenated_features[:, 240:756, :], highest_targets)\n",
        "\n",
        "rf_lowest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_lowest.fit(concatenated_features[:, 240:756, :], lowest_targets)\n",
        "\n",
        "# predict intraday returns for testing period and rank stocks based on predicted returns\n",
        "test_features = concatenated_features[:, 756:, :]\n",
        "highest_predictions = rf_highest.predict(test_features)\n",
        "highest_ranks = np.argsort(-highest_predictions)[:k]\n",
        "\n",
        "lowest_predictions = rf_lowest.predict(test_features)\n",
        "lowest_ranks = np.argsort(lowest_predictions)[:k]\n",
        "\n",
        "# print the k stocks with highest/lowest predicted intraday returns\n",
        "print(\"Top {} stocks with highest predicted intraday returns: {}\".format(k, highest_indices[highest_ranks]))\n",
        "print(\"Top {} stocks with lowest predicted intraday returns: {}\".format(k, lowest_indices[lowest_ranks]))\n",
        "```\n",
        "\n",
        "Next Solution, I got when I mentioned to use containers and targets,\n",
        "```python\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import numpy as np\n",
        "\n",
        "# Load data\n",
        "containers = []\n",
        "targets = []\n",
        "for i in range(26):\n",
        "    container_i = np.load(f\"features_{i}.npy\")\n",
        "    target_i = np.load(f\"targets_{i}.npy\")\n",
        "    containers.append(container_i)\n",
        "    targets.append(target_i)\n",
        "\n",
        "# Combine all containers and targets\n",
        "X_train = np.concatenate([container[:,240:756,:] for container in containers], axis=0)\n",
        "y_train = np.concatenate([target[:,240:756,:] for target in targets], axis=0)\n",
        "X_test = np.concatenate([container[:,756:,:] for container in containers], axis=0)\n",
        "y_test = np.concatenate([target[:,756:,:] for target in targets], axis=0)\n",
        "\n",
        "# Convert y_train and y_test to 1D arrays\n",
        "y_train = y_train.flatten()\n",
        "y_test = y_test.flatten()\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# Reshape y_test and y_pred to original shape\n",
        "y_test = y_test.reshape(-1, 1008, 1)\n",
        "y_pred = y_pred.reshape(-1, 1008, 1)\n",
        "\n",
        "# Calculate intraday returns\n",
        "intraday_returns = np.divide(np.subtract(y_test[:,1:,:], y_test[:,:-1,:]), y_test[:,:-1,:])\n",
        "intraday_returns_pred = np.divide(np.subtract(y_pred[:,1:,:], y_pred[:,:-1,:]), y_pred[:,:-1,:])\n",
        "\n",
        "# Calculate mean intraday return for each stock\n",
        "mean_intraday_returns = np.mean(intraday_returns, axis=1)\n",
        "mean_intraday_returns_pred = np.mean(intraday_returns_pred, axis=1)\n",
        "\n",
        "# Get indices of top k and bottom k mean intraday returns\n",
        "k = 5\n",
        "top_k_idx = np.argsort(mean_intraday_returns_pred[:,0])[-k:]\n",
        "bottom_k_idx = np.argsort(mean_intraday_returns_pred[:,0])[:k]\n",
        "\n",
        "# Get the names of the stocks with top k and bottom k mean intraday returns\n",
        "# Assuming that you have a file 'stock_names.txt' with names of all 251 stocks\n",
        "with open('stock_names.txt', 'r') as f:\n",
        "    stock_names = f.readlines()\n",
        "    stock_names = [x.strip() for x in stock_names]\n",
        "\n",
        "top_k_stock_names = [stock_names[i] for i in top_k_idx]\n",
        "bottom_k_stock_names = [stock_names[i] for i in bottom_k_idx]\n",
        "\n",
        "# Print the names of the stocks with top k and bottom k mean intraday returns\n",
        "print(f\"Top {k} stocks with highest mean intraday returns:\")\n",
        "for name in top_k_stock_names:\n",
        "    print(name)\n",
        "print(f\"\\nBottom {k} stocks with lowest mean intraday returns:\")\n",
        "for name in bottom_k_stock_names:\n",
        "    print(name)\n",
        "\n",
        "```\n",
        "\n",
        "Based on your new requirement, it sounds like we need to generate features for a single day t using the opening prices of the first t days and the closing prices of the first t-1 days. We can do this by creating a function that takes the opening and closing prices as input and returns the 93 features.\n",
        "\n",
        "Here's a modified version of the previous solution that takes this into account:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "def generate_features(open_prices, close_prices):\n",
        "    # Assume open_prices and close_prices are numpy arrays of shape (n_stocks, t) and (n_stocks, t-1)\n",
        "    # respectively.\n",
        "    # Generate 93 features for each stock for the given day t.\n",
        "    features = ... # your feature generation code here\n",
        "    return features\n",
        "\n",
        "# Assume you have a list of 26 numpy arrays called \"containers\", each of shape (n_stocks, n_days, n_features)\n",
        "# containing the features for each day's stock data, and a list of 26 numpy arrays called \"targets\", each of\n",
        "# shape (n_stocks, n_days, 1) containing the intraday returns for each day's stock data.\n",
        "\n",
        "# We'll use the first 25 datasets to train the model and the last dataset for testing/trading.\n",
        "train_containers = containers[:-1]\n",
        "train_targets = targets[:-1]\n",
        "test_container = containers[-1]\n",
        "test_target = targets[-1]\n",
        "\n",
        "# Generate features for the training data.\n",
        "train_features = []\n",
        "for i in range(len(train_containers)):\n",
        "    container = train_containers[i]\n",
        "    target = train_targets[i]\n",
        "    # Remove the first year of data (used for feature creation).\n",
        "    container = container[:, 252:, :]\n",
        "    target = target[:, 252:, :]\n",
        "    # Split into training and validation sets.\n",
        "    X_train = container[:, :504, :]\n",
        "    y_train = target[:, :504, :]\n",
        "    X_val = container[:, 504:756, :]\n",
        "    y_val = target[:, 504:756, :]\n",
        "    # Generate features for each day in the training set.\n",
        "    for j in range(X_train.shape[1]):\n",
        "        open_prices = X_train[:, j, :4] # Use the opening prices of the first 4 days.\n",
        "        close_prices = X_train[:, j, 4:8] # Use the closing prices of the first 4 days.\n",
        "        features = generate_features(open_prices, close_prices)\n",
        "        train_features.append(features)\n",
        "    # Generate features for each day in the validation set.\n",
        "    for j in range(X_val.shape[1]):\n",
        "        open_prices = X_val[:, j, :4] # Use the opening prices of the first 4 days.\n",
        "        close_prices = X_val[:, j, 4:8] # Use the closing prices of the first 4 days.\n",
        "        features = generate_features(open_prices, close_prices)\n",
        "        train_features.append(features)\n",
        "\n",
        "# Generate features for the test data.\n",
        "test_features = []\n",
        "# Remove the first year of data (used for feature creation).\n",
        "test_container = test_container[:, 252:, :]\n",
        "test_target = test_target[:, 252:, :]\n",
        "# Generate features for each day in the test set.\n",
        "for j in range(test_container.shape[1]):\n",
        "    open_prices = test_container[:, j, :4] # Use the opening prices of the first 4 days.\n",
        "    close_prices = test_container[:, j, 4:8] # Use the closing prices of the first 4 days.\n",
        "    features = generate_features(open_prices, close_prices)\n",
        "    test_features.append(features)\n",
        "\n",
        "# Convert the feature and target lists to numpy arrays.\n",
        "# Convert the feature and target lists to numpy arrays.\n",
        "X_train = np.concatenate(train_containers, axis=0)\n",
        "X_test = np.concatenate(test_containers, axis=0)\n",
        "y_train = np.concatenate(train_targets, axis=0).reshape(-1)\n",
        "y_test = np.concatenate(test_targets, axis=0).reshape(-1)\n",
        "\n",
        "# Fit the random forest classifier on the training data.\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict the intraday returns for the test data.\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# Reshape the target arrays back to their original shape.\n",
        "y_test = y_test.reshape(len(test_targets), -1, 1)\n",
        "y_pred = y_pred.reshape(len(test_targets), -1, 1)\n",
        "\n",
        "# Select the top k and bottom k stocks based on their intraday return on the prediction day.\n",
        "prediction_day = 756\n",
        "k = 10\n",
        "top_k_stocks = np.argsort(y_pred[:, prediction_day, 0])[::-1][:k]\n",
        "bottom_k_stocks = np.argsort(y_pred[:, prediction_day, 0])[:k]\n",
        "\n",
        "# Print the top k and bottom k stocks.\n",
        "print(f\"Top {k} stocks with highest intraday return on day {prediction_day}:\")\n",
        "for stock_index in top_k_stocks:\n",
        "    print(f\"Stock index: {stock_index}, intraday return: {y_pred[stock_index, prediction_day, 0]}\")\n",
        "    \n",
        "print(f\"Bottom {k} stocks with lowest intraday return on day {prediction_day}:\")\n",
        "for stock_index in bottom_k_stocks:\n",
        "    print(f\"Stock index: {stock_index}, intraday return: {y_pred[stock_index, prediction_day, 0]}\")\n",
        "```\n",
        "\n",
        "```\n",
        "Let's say for example the first dataset among the 26 datasets starts from year 1990 and ends on 1993, 4 years window size. Now, in this approach we are considering 1990's stock data to generete features for the model, 1991 and 1992's data is used for training the model and 1993's stock to test the model. We do it for all other 25 datasets. Now how will you train the model using this type of 26 datasets. Also perform the same objective like before.\n",
        "```\n",
        "\n",
        "To train the model using 26 datasets where each dataset contains 4 years of stock data, we can follow a similar approach as before. However, we need to make some modifications to account for the fact that each dataset contains a 4-year window of data.\n",
        "\n",
        "Here's a modified code snippet to load the data and split it into training and testing sets for each dataset:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Load the data and split it into 26 datasets\n",
        "# Each dataset contains 4 years of stock data\n",
        "data = np.load('data.npy')\n",
        "n_datasets = 26\n",
        "dataset_size = data.shape[1] // n_datasets\n",
        "datasets = [data[:, i*dataset_size:(i+1)*dataset_size, :] for i in range(n_datasets)]\n",
        "\n",
        "# Split each dataset into training and testing sets\n",
        "train_size = 2 * 252  # 2 years of data for training\n",
        "test_size = 252  # 1 year of data for testing\n",
        "X_train_list, y_train_list, X_test_list, y_test_list = [], [], [], []\n",
        "for dataset in datasets:\n",
        "    X_train, y_train = [], []\n",
        "    X_test, y_test = [], []\n",
        "    for i in range(dataset_size - train_size - test_size):\n",
        "        # Use data from first year for feature creation\n",
        "        features = generate_features_rf(dataset[:, i:i+train_size+test_size, :])\n",
        "        X = features[0][252:,:]  # Use data from second year onwards for training/testing\n",
        "        y = features[1][252:,:]\n",
        "        X_train.append(X[:train_size, :])\n",
        "        y_train.append(y[:train_size, :])\n",
        "        X_test.append(X[train_size:, :])\n",
        "        y_test.append(y[train_size:, :])\n",
        "    X_train_list.append(np.array(X_train))\n",
        "    y_train_list.append(np.array(y_train))\n",
        "    X_test_list.append(np.array(X_test))\n",
        "    y_test_list.append(np.array(y_test))\n",
        "\n",
        "```\n",
        "\n",
        "Next, we can train the random forest model on all 26 datasets at once using the `multi_output=True` parameter in `RandomForestRegressor`:\n",
        "\n",
        "```python\n",
        "# Train a random forest model on all 26 datasets at once\n",
        "rf = RandomForestRegressor(n_estimators=100, n_jobs=-1, random_state=42, multi_output=True)\n",
        "X_train_all = np.concatenate(X_train_list, axis=0)\n",
        "y_train_all = np.concatenate(y_train_list, axis=0)\n",
        "rf.fit(X_train_all, y_train_all)\n",
        "```\n",
        "\n",
        "To make predictions for a given day `t`, we can use the same approach as before. However, we need to modify the feature creation process to only use the opening and closing prices up to day `t-1`, since we cannot use the stock prices for day `t` in feature creation:\n",
        "\n",
        "```python\n",
        "# Load the opening and closing prices up to day t-1 for all stocks\n",
        "prices = np.load('prices.npy')\n",
        "prices = prices[:, :t, :]\n",
        "\n",
        "# Use the opening and closing prices to generate features for day t\n",
        "features = generate_features_rf(prices)\n",
        "X = features[0][-1:, :]  # Use the last day's features for prediction\n",
        "y_pred = rf.predict(X)\n",
        "\n",
        "# Get the indices of the k stocks with highest and lowest predicted intraday return\n",
        "k = 10\n",
        "top_k = np.argsort(y_pred.flatten())[-k:]\n",
        "bottom_k = np.argsort(y_pred.flatten())[:k]\n",
        "```\n",
        "Here, `prices` is a numpy array of shape `(n_stocks, t, 2)`, where the last dimension corresponds to the opening and closing prices for each day up to day `t-1`. We use `generate_features`."
      ],
      "metadata": {
        "id": "HucybgOtCrv3"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}