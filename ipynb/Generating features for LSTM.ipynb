{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96c10457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For data processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load pickle file\n",
    "import pickle\n",
    "\n",
    "# LSTM and other layers\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d68aa25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../datasets/datasets-list', 'rb') as file:\n",
    "    datasets = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170aa524",
   "metadata": {},
   "source": [
    "### Feature generation for LSTM\n",
    "\n",
    "We input the model with 240 timesteps and 3 features and train it to predict the direction of the $241^{st}$ intraday return.\n",
    "\n",
    "More precisely, for each stock $s$ at time $t$, we first consider the following three features $ir^{(s)}_{t, 1}, cr^{(s)}_{t, 1}, or^{(s)}_{t, 1}$ defined above.\n",
    "\n",
    "Then we apply the Robust Scaler Standardization\n",
    "\n",
    "$\\tilde f^{(s)}_{t, 1} := \\dfrac {f^{(s)}_{t,1} - Q_2(f^{(s)}_{.,1})} {Q_3(f^{(s)}_{.,1}) - Q_1(f^{(s)}_{.,1})}$\n",
    "\n",
    "where $Q_1(f^{(s)}_{.,1}), Q_2(f^{(s)}_{.,1})$ and $Q_3(f^{(s)}_{.,1})$ are the first, second and third quartile of $f^{(s)}_{.,1}$, for each feature $f^{(s)}_{.,1} \\in \\{ ir^{(s)}_{., 1}, cr^{(s)}_{., 1}, or^{(s)}_{., 1} \\}$ in the respective training period.\n",
    "\n",
    "The Robust Scaler Standardization first subtracts (and hence removes) the median and then scales the data using the inter-quartile range, making it robust to outliers.\n",
    "\n",
    "Next for each time $t \\in \\{ 240, 241, ..., T_{study} \\}$, we generate overlapping sequence of 240 consecutive, three-dimensional standardized features $\\{ \\tilde F^{(s)}_{t-239,1}, \\tilde F^{(s)}_{t-238,1}, ..., \\tilde F^{(s)}_{t,1} \\}$, where $\\tilde F^{(s)}_{t-i,1} := (\\tilde ir^{(s)}_{t-i,1}, \\tilde cr^{(s)}_{t-i,1}, \\tilde or^{(s)}_{t-i,1}), i \\in \\{ 239, 238, ..., 0 \\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "541a8e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_ir_cr_or(curr_dataset):\n",
    "    t_study = curr_dataset.shape[1]\n",
    "    time = np.arange(0, t_study) # time starts from 0 to T_study-1\n",
    "    \n",
    "    # Seperate the cp and op for each stock price\n",
    "    cp_S = curr_dataset[:, :, 0]\n",
    "    op_S = curr_dataset[:, :, 1]\n",
    "    \n",
    "    # time for calculation of ir starts from time=1, so time[1:]\n",
    "    cp_t_1 = cp_S[:, time[1:]-1]\n",
    "    op_t_1 = op_S[:, time[1:]-1]\n",
    "    # Create a container for ir with the same size as cp_S or op_S\n",
    "    ir_t_1 = np.zeros(shape=cp_S.shape)\n",
    "    # Fill the values as nan as the first value will be NaN except all others because for t=0, m=1 is negative\n",
    "    ir_t_1[:, :] = np.nan\n",
    "    # Divide cp_t_1 by op_t_1, use np.divide to avoid DivideByZero exception. First ir is at time=1 index.\n",
    "    ir_t_1[:, time[1]:] = np.divide(cp_t_1, op_t_1, out=np.zeros_like(cp_t_1), where=op_t_1!=0) - 1\n",
    "    \n",
    "    # time for calculation of cr starts from time=2, so time[2:]\n",
    "    cp_t_1 = cp_S[:, time[2:]-1]\n",
    "    cp_t_2 = cp_S[:, time[2:]-2]\n",
    "    # Create a container for cr with the same size as cp_S or op_S\n",
    "    cr_t_1 = np.zeros(shape=cp_S.shape)\n",
    "    #Fill all values with NaN\n",
    "    cr_t_1[:, :] = np.nan\n",
    "    # Divide cp_t_1 by cp_t_1_1 or cp_t_2, use np.divide to avoud DivideByZero exception. First cr is at time=2 index.\n",
    "    cr_t_1[:, time[2]:] = np.divide(cp_t_1, cp_t_2, out=np.zeros_like(cp_t_1), where=cp_t_2!=0) - 1\n",
    "    \n",
    "    # time for calculating or starts from time=1, so time[1:]\n",
    "    cp_t_1 = cp_S[:, time[1:]-1]\n",
    "    op_t = op_S[:, time[1:]]\n",
    "    # Create a container for cr with the same size as cp_S or op_S\n",
    "    or_t_1 = np.zeros(shape=cp_S.shape)\n",
    "    #Fill all values with NaN\n",
    "    or_t_1[:, :] = np.nan\n",
    "    # Divide cp_t_1 by cp_t_1_1 or cp_t_2, use np.divide to avoud DivideByZero exception. First cr is at time=2 index.\n",
    "    or_t_1[:, time[1]:] = np.divide(op_t, cp_t_1, out=np.zeros_like(op_t), where=cp_t_1!=0) - 1\n",
    "    \n",
    "    \n",
    "    \n",
    "    return [ir_t_1, cr_t_1, or_t_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8dd3235a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_scaler(fs):\n",
    "    # Create a dummy with the original size\n",
    "    dummy = np.zeros_like(fs)\n",
    "    dummy[:, :] = np.nan\n",
    "    \n",
    "    fs = fs[:, 1:] # Remove the 0th day as it contains NaN\n",
    "    \n",
    "    Q1 = np.percentile(fs, 25, axis=1)\n",
    "    Q2 = np.percentile(fs, 50, axis=1)\n",
    "    Q3 = np.percentile(fs, 75, axis=1)\n",
    "    \n",
    "    scaled = (fs - Q2.reshape(-1, 1))\n",
    "    inter_qrtl_range = (Q3.reshape(-1, 1) - Q1.reshape(-1, 1))\n",
    "    \n",
    "    scaled_fs = np.divide(scaled, inter_qrtl_range, out=np.zeros_like(scaled), where = inter_qrtl_range!=0)\n",
    "    \n",
    "    # Store the scaled values inside non-nan positions\n",
    "    dummy[:, 1:] = scaled_fs\n",
    "    \n",
    "    return dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "051cba65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features_lstm(dataset):\n",
    "    n_stocks = dataset.shape[0]\n",
    "    \n",
    "    # ir, cr and or will be returned after calling calc_ir_cr_or function\n",
    "    three_features = calc_ir_cr_or(dataset)\n",
    "    \n",
    "    ir_t1 = three_features[0]\n",
    "    cr_t1 = three_features[1]\n",
    "    or_t1 = three_features[2]\n",
    "    \n",
    "    scaled_ir = robust_scaler(ir_t1)\n",
    "    scaled_cr = robust_scaler(cr_t1)\n",
    "    scaled_or = robust_scaler(or_t1)\n",
    "    \n",
    "    T_study = dataset.shape[1]\n",
    "    T = np.arange(241, T_study) # t in {241, 242, ...., T_study}\n",
    "    I = np.arange(240, -1, -1) # i in {240, 239, ...., 0}\n",
    "    \n",
    "    # F_scaled = {scaled_ir, scaled_cr, scaled_or}; F_scaled_(t,1) is the target variable\n",
    "    # Data: F_scaled_(1, 1), F_scaled_(2, 1), F_scaled_(3, 1), ...., F_scaled_(240, 1), F_scaled_(241, 1)\n",
    "    # Data: F_scaled_(2, 1), F_scaled_(3, 1), F_scaled_(4, 1), ...., F_scaled_(241, 1), F_scaled_(242, 1)\n",
    "    # Data: F_scaled_(3, 1), F_scaled_(4, 1), F_scaled_(5, 1), ...., F_scaled_(242, 1), F_scaled_(243, 1)\n",
    "    # ....\n",
    "    # ....\n",
    "    # ....\n",
    "    # Data: F_scaled_(t-i, 1) for i in {240, 239, ..., 0} Target: F_scaled_(t, 1)\n",
    "    \n",
    "    features = []\n",
    "    for t in T:\n",
    "        scaled_ir_ti = scaled_ir[:, t - I]\n",
    "        scaled_cr_ti = scaled_cr[:, t - I]\n",
    "        scaled_or_ti = scaled_or[:, t - I]\n",
    "        \n",
    "        F_scaled = np.array([scaled_ir_ti, scaled_cr_ti, scaled_or_ti]).reshape(n_stocks, -1, 3)\n",
    "        features.append(F_scaled)\n",
    "        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029b90c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_container = []\n",
    "for dataset in datasets:\n",
    "    features_container.append(np.array(generate_features_lstm(dataset))) \n",
    "    \n",
    "# MEMORY PROBLEM!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
