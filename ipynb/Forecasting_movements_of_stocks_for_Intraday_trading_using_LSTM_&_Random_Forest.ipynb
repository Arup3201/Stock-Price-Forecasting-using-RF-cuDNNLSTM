{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We input the model with 240 timesteps and 3 features and train it to predict the direction of the $241^{st}$ intraday return.\n",
    "\n",
    "More precisely, for each stock $s$ at time $t$, we first consider the following three features $ir^{(s)}_{t, 1}, cr^{(s)}_{t, 1}, or^{(s)}_{t, 1}$ defined above.\n",
    "\n",
    "Then we apply the Robust Scaler Standardization\n",
    "\n",
    "$\\tilde f^{(s)}_{t, 1} := \\dfrac {f^{(s)}_{t,1} - Q_2(f^{(s)}_{.,1})} {Q_3(f^{(s)}_{.,1}) - Q_1(f^{(s)}_{.,1})}$\n",
    "\n",
    "where $Q_1(f^{(s)}_{.,1}), Q_2(f^{(s)}_{.,1})$ and $Q_3(f^{(s)}_{.,1})$ are the first, second and third quartile of $f^{(s)}_{.,1}$, for each feature $f^{(s)}_{.,1} \\in \\{ ir^{(s)}_{., 1}, cr^{(s)}_{., 1}, or^{(s)}_{., 1} \\}$ in the respective training period.\n",
    "\n",
    "The Robust Scaler Standardization first subtracts (and hence removes) the median and then scales the data using the inter-quartile range, making it robust to outliers.\n",
    "\n",
    "Next for each time $t \\in \\{ 240, 241, ..., T_{study} \\}$, we generate overlapping sequence of 240 consecutive, three-dimensional standardized features $\\{ \\tilde F^{(s)}_{t-239,1}, \\tilde F^{(s)}_{t-238,1}, ..., \\tilde F^{(s)}_{t,1} \\}$, where $\\tilde F^{(s)}_{t-i,1} := (\\tilde ir^{(s)}_{t-i,1}, \\tilde cr^{(s)}_{t-i,1}, \\tilde or^{(s)}_{t-i,1}), i \\in \\{ 239, 238, ..., 0 \\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Order of values : Adj Close, Open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DBZJPg65wYKT",
    "outputId": "112c68e8-df8c-4ae2-807b-d17d8508aff2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(251, 1013)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DEMO CODE üëá\n",
    "def robust_scaler():\n",
    "    [fs, _, _] = calc_ir_cr_or(datasets[0])\n",
    "    \n",
    "    return fs.shape\n",
    "\n",
    "robust_scaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "25AbDpT-SQCk"
   },
   "source": [
    "#  Forecasting directional movements of stock prices for intraday trading using Random Forest & CuDNNLSTM\n",
    "\n",
    "-----------------------------------------------------------\n",
    "*What Is Intraday Return?*\n",
    "\n",
    "The intraday return is one of the two components of the total daily return generated by a stock. Intraday return measures the return generated by a stock during regular trading hours, based on its price change from the opening of a trading day to its close. Intraday return and overnight return together constitute the total daily return from a stock, which is based on the price change of a stock from the close of one trading day to the close of the next trading day. It is also called daytime return.\n",
    "\n",
    "Intraday return is of particular importance for day traders, who use daytime gyrations in stocks and markets to make trading profits, and rarely leave positions open overnight. Day trading strategies are not as commonplace for regular investors as they were before the 2008-2009 recession.\n",
    "\n",
    "---------------------------------------------------------\n",
    "*How to Calculate Daily Returns?*\n",
    "\n",
    "To calculate a daily return, you subtract the starting price from the closing price. Once you have that, you simply multiply by the number of shares you own.\n",
    "\n",
    "To illustrate, let's say you own `100` shares of XYZ stock. The day opens at `$20` and closes at `$25`. This is a `$5` positive difference. Multiply the `$5` difference by the `100` shares you own, for a daily return of `$500`.\n",
    "\n",
    "Some investors will prefer to work in percentages rather than dollar amounts. This is only slightly more complicated. You perform the same first step and arrive at a `$5` gain per share for the day. You then divide by the opening price of `$25`, leaving you with `0.2`. Multiply by `100` to arrive at your daily return of `20%`.\n",
    "\n",
    "-----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A7HVMz79RJ8v"
   },
   "source": [
    "This project is done by following the methods and techniques of the paper `Forecasting directional movements of stock prices for intraday trading using LSTM and random forests`. Link to the paper: [Click Here](https://arxiv.org/pdf/2004.10178.pdf).\n",
    "\n",
    "This introduces multi-feature setting consisting not only of the returns with respect to the closing prices, but also with respect to the opening prices and intraday returns to predict each stock, at the beginning of each day, the probablity to outperform the market in terms of intraday returns.\n",
    "\n",
    "As dataset we use all stocks of the S&P 500 from the period of January 1990 until December 2018.\n",
    "\n",
    "We employ both Random Forests on the one hand and LSTM on the other hand as training methodology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UyxAjLk6cXMc"
   },
   "source": [
    "### Technology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VOFu4PhEdkK3"
   },
   "source": [
    "- Python: 3.9.16\n",
    "- Scikit-Learn: 1.2.2\n",
    "- Tensorflow: 2.12.0\n",
    "- System RAM: 12.7 GB\n",
    "- GPU RAM: 15.0 GB\n",
    "- Disk: 78.2 GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7JsDmuufYS1K"
   },
   "source": [
    "## Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jErs_PBbSjM9"
   },
   "outputs": [],
   "source": [
    "# For data processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load pickle file\n",
    "import pickle\n",
    "\n",
    "# For Random Forest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# LSTM and other layers\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EmhPxpT6t8HK"
   },
   "source": [
    "## Download the stocks and save it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to `Collect Stock Data and Store it.ipynb` file to download the stocks data and save it.\n",
    "\n",
    "Then come back to this file. But if you have already downloaded the data by running that file once then no need to run it again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0GbRIIBKfUGT"
   },
   "source": [
    "## Steps to follow - \n",
    "1. We divide our raw data into study periods, where each study period is divided into a training part(for in-sample trading) and a trading part(for out-sample predictions).\n",
    "2. We introduce out features.\n",
    "3. We set up our targets.\n",
    "4. We define our 2 machine learning methods we employ, namely random forest and CuDNNLSTM.\n",
    "5. Establish a trading strategy for trading part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gsitj69EeR0U"
   },
   "source": [
    "## Data preparation for Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBHNC--xEvo5"
   },
   "source": [
    "\n",
    "### Preparing the original dataset for further processing\n",
    "\n",
    "*Segregate the stocks within different numpy arrays according to the ticker name.*\n",
    "![Stack-stock-data-on-top-of-each-other.png](https://i.postimg.cc/HnDyJp9s/Stack-stock-data-on-top-of-each-other.png)\n",
    "\n",
    "WARNING‚ùó \n",
    "- Different stocks will give different no. of rows, as all stocks were not always available in that time span.\n",
    "- Here we are not able to properly get all days values.\n",
    "\n",
    "To prevent the error, we are removing those stocks will are having empty values for those dates.\n",
    "\n",
    "E.g. Stock A might start from 1991-01-01 and Stock B from 1990-12-31 then both of them are not in same shape. So we can remove stock B. Making sure all stocks are having same no of rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9luoTa3nnVa8"
   },
   "source": [
    "### Datasets creation with non-overlapping testing period from original dataset\n",
    "\n",
    "We divide the dataset contsisting of 29 years starting from January 1990 till December 2018, using a 4-year window, 1-year stride, where each study period is divided into a training part(of 756 days almost = 3 years) and trading part(of 252 days almost = 1 year).\n",
    "\n",
    "So, we obtain 26 study periods with non-overlapping trading part.\n",
    "\n",
    "![Dataset-creation-with-non-overlapping-testing-period.png](https://i.postimg.cc/7YWw5YwL/Dataset-creation-with-non-overlapping-testing-period.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BNa54mX2juGf"
   },
   "source": [
    "Refer to `Generating List of Datasets and Saving it.ipynb` file for the steps of `Preparing the original dataset for further processing` and `Datasets creation with non-overlapping testing period from original dataset`.\n",
    "\n",
    "After running all the cells, come back this file. Now you are ready to run the below cells as it is loading the `datasets-list` pickle file saved by `Generating List of Datasets and Saving it.ipynb` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcJwB422SGMb"
   },
   "source": [
    "## Features Selection\n",
    "\n",
    "Let $T_{study}$ denote the total amount of days in a study period and $n_i$ represent the number of stocks $s$ in $S$ having complete historical data available at the end of each study period $i$. Moreover, we define the adjacent closing price and opening price of any stock $s \\in S$ at time $t$ by $cp^{(s)}_t$ and $op^{(s)}_t$.\n",
    "\n",
    "Given a prediction day $t:=\\tau$, we have the following inputs and prediction task.\n",
    "\n",
    "Input: We have the historical opening prices, $op^{(s)}_t, t \\in \\{ 0, 1, ..., \\tau -1, \\tau\\}$, (including the opening price of the prediction day $op^{(s)}_\\tau$) as well as the historical adjacent closing prices, $cp^{(s)}_t, t \\in \\{ 0, 1, ..., \\tau -1\\}$, (excluding the opening price of the prediction day $cp^{(s)}_\\tau$).\n",
    "\n",
    "Task: Out of all n stocks, predict k stocks with highest and k stocks with lowest intraday return $ir_{\\tau, 0} = \\dfrac{cp_\\tau}{op_\\tau} - 1$.\n",
    "\n",
    "**NOTE:** In the original paper they used all the stocks that could be scrapped from the web. Then they divided each stock into 26 datasets. Now, in this 26 datasets, some datasets may contain all 492 stocks that were originally scrapped and some datasets may contain only 251 stocks. That is why it is saying $s \\in S$ because each dataset will have different number of stocks and that will be a subset of all the originally scrapped stocks.\n",
    "\n",
    "But, in our case we are dealing with only those stocks which has all entries filled from 1990-01-02 to 2018-12-31. So, we have 251 stocks in all the datasets.\n",
    "\n",
    "\n",
    "\n",
    "For LaTex markdown, refer to this page: [here](https://ashki23.github.io/markdown-latex.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dus29JN9SJUZ"
   },
   "source": [
    "### Feature generation for Random Forest\n",
    "\n",
    "For any stock $s \\in S$ and any time $t \\in \\{ 241, 242, ..., T_{study} \\}$, the feature set we provide to the random forest comprises of 3 signal:\n",
    "\n",
    "1. Intraday return: $ir^{(s)}_{t, m} := \\dfrac{cp^{(s)}_{t-m}}{op^{(s)}_{t-m}} - 1$,\n",
    "\n",
    "\n",
    "2. Returns with respect to last closing price: $cr^{(s)}_{t, m} := \\dfrac{cp^{(s)}_{t-1}}{cp^{(s)}_{t-1-m}} - 1$,\n",
    "\n",
    "\n",
    "3. Returns with respect to opening price: $or^{(s)}_{t, m} := \\dfrac{op^{(s)}_{t}}{cp^{(s)}_{t-m}} - 1$,\n",
    "\n",
    "where $m \\in \\{ 1, 2, 3, ..., 20 \\} \\cup \\{ 40, 60, 80, ...., 240 \\}$, obtaining 93 features. By the choice of m we consider in the first month the corresponding returns of each trading day, whereas for the subsequent 11 months we only consider the corresponding multi-period returns of each month.\n",
    "\n",
    "![ir-cr-and-or-calculation.png](https://i.postimg.cc/T3sjRDQ1/ir-cr-and-or-calculation.png)\n",
    "\n",
    "Refer to the `Generating features for Random Forest.ipynb` file for the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PvuESLpv1-nq"
   },
   "source": [
    "### Feature generation for LSTM\n",
    "\n",
    "We input the model with 240 timesteps and 3 features and train it to predict the direction of the $241^{st}$ intraday return.\n",
    "\n",
    "More precisely, for each stock $s$ at time $t$, we first consider the following three features $ir^{(s)}_{t, 1}, cr^{(s)}_{t, 1}, or^{(s)}_{t, 1}$ defined above.\n",
    "\n",
    "Then we apply the Robust Scaler Standardization\n",
    "\n",
    "$\\tilde f^{(s)}_{t, 1} := \\dfrac {f^{(s)}_{t,1} - Q_2(f^{(s)}_{.,1})} {Q_3(f^{(s)}_{.,1}) - Q_1(f^{(s)}_{.,1})}$\n",
    "\n",
    "where $Q_1(f^{(s)}_{.,1}), Q_2(f^{(s)}_{.,1})$ and $Q_3(f^{(s)}_{.,1})$ are the first, second and third quartile of $f^{(s)}_{.,1}$, for each feature $f^{(s)}_{.,1} \\in \\{ ir^{(s)}_{., 1}, cr^{(s)}_{., 1}, or^{(s)}_{., 1} \\}$ in the respective training period.\n",
    "\n",
    "The Robust Scaler Standardization first subtracts (and hence removes) the median and then scales the data using the inter-quartile range, making it robust to outliers.\n",
    "\n",
    "Next for each time $t \\in \\{ 240, 241, ..., T_{study} \\}$, we generate overlapping sequence of 240 consecutive, three-dimensional standardized features $\\{ \\tilde F^{(s)}_{t-239,1}, \\tilde F^{(s)}_{t-238,1}, ..., \\tilde F^{(s)}_{t,1} \\}$, where $\\tilde F^{(s)}_{t-i,1} := (\\tilde ir^{(s)}_{t-i,1}, \\tilde cr^{(s)}_{t-i,1}, \\tilde or^{(s)}_{t-i,1}), i \\in \\{ 239, 238, ..., 0 \\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../datasets/datasets-list', 'rb') as file:\n",
    "    datasets = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_ir_cr_or(curr_dataset):\n",
    "    t_study = curr_dataset.shape[1]\n",
    "    time = np.arange(0, t_study) # time starts from 0 to T_study-1\n",
    "    \n",
    "    # Seperate the cp and op for each stock price\n",
    "    cp_S = curr_dataset[:, :, 0]\n",
    "    op_S = curr_dataset[:, :, 1]\n",
    "    \n",
    "    # time for calculation of ir starts from time=1, so time[1:]\n",
    "    cp_t_1 = cp_S[:, time[1:]-1]\n",
    "    op_t_1 = op_S[:, time[1:]-1]\n",
    "    # Create a container for ir with the same size as cp_S or op_S\n",
    "    ir_t_1 = np.zeros(shape=cp_S.shape)\n",
    "    # Fill the values as nan as the first value will be NaN except all others because for t=0, m=1 is negative\n",
    "    ir_t_1[:, :] = np.nan\n",
    "    # Divide cp_t_1 by op_t_1, use np.divide to avoid DivideByZero exception. First ir is at time=1 index.\n",
    "    ir_t_1[:, time[1]:] = np.divide(cp_t_1, op_t_1, out=np.zeros_like(cp_t_1), where=op_t_1!=0) - 1\n",
    "    \n",
    "    # time for calculation of cr starts from time=2, so time[2:]\n",
    "    cp_t_1 = cp_S[:, time[2:]-1]\n",
    "    cp_t_2 = cp_S[:, time[2:]-2]\n",
    "    # Create a container for cr with the same size as cp_S or op_S\n",
    "    cr_t_1 = np.zeros(shape=cp_S.shape)\n",
    "    #Fill all values with NaN\n",
    "    cr_t_1[:, :] = np.nan\n",
    "    # Divide cp_t_1 by cp_t_1_1 or cp_t_2, use np.divide to avoud DivideByZero exception. First cr is at time=2 index.\n",
    "    cr_t_1[:, time[2]:] = np.divide(cp_t_1, cp_t_2, out=np.zeros_like(cp_t_1), where=cp_t_2!=0) - 1\n",
    "    \n",
    "    # time for calculating or starts from time=1, so time[1:]\n",
    "    cp_t_1 = cp_S[:, time[1:]-1]\n",
    "    op_t = op_S[:, time[1:]]\n",
    "    # Create a container for cr with the same size as cp_S or op_S\n",
    "    or_t_1 = np.zeros(shape=cp_S.shape)\n",
    "    #Fill all values with NaN\n",
    "    or_t_1[:, :] = np.nan\n",
    "    # Divide cp_t_1 by cp_t_1_1 or cp_t_2, use np.divide to avoud DivideByZero exception. First cr is at time=2 index.\n",
    "    or_t_1[:, time[1]:] = np.divide(op_t, cp_t_1, out=np.zeros_like(op_t), where=cp_t_1!=0) - 1\n",
    "    \n",
    "    \n",
    "    \n",
    "    return [ir_t_1, cr_t_1, or_t_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(251, 1013, 3)\n",
      "(251, 1012, 3)\n",
      "(251, 1011, 3)\n",
      "(251, 1011, 3)\n",
      "(251, 1011, 3)\n",
      "(251, 1011, 3)\n",
      "(251, 1011, 3)\n",
      "(251, 1009, 3)\n",
      "(251, 1004, 3)\n",
      "(251, 1004, 3)\n",
      "(251, 1004, 3)\n",
      "(251, 1004, 3)\n",
      "(251, 1008, 3)\n",
      "(251, 1007, 3)\n",
      "(251, 1006, 3)\n",
      "(251, 1007, 3)\n",
      "(251, 1007, 3)\n",
      "(251, 1008, 3)\n",
      "(251, 1009, 3)\n",
      "(251, 1006, 3)\n",
      "(251, 1006, 3)\n",
      "(251, 1006, 3)\n",
      "(251, 1006, 3)\n",
      "(251, 1008, 3)\n",
      "(251, 1007, 3)\n",
      "(251, 1005, 3)\n"
     ]
    }
   ],
   "source": [
    "def generate_features_lstm(dataset):\n",
    "    n_stocks = 251   \n",
    "    \n",
    "    ir_cr_or = calc_ir_cr_or(dataset)\n",
    "    three_features = np.array(ir_cr_or).reshape(n_stocks, -1, 3)\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "7JsDmuufYS1K",
    "Gzrx5Q7DYixY",
    "oBHNC--xEvo5",
    "9luoTa3nnVa8",
    "dus29JN9SJUZ"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "stock-kernel",
   "language": "python",
   "name": "stock-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
