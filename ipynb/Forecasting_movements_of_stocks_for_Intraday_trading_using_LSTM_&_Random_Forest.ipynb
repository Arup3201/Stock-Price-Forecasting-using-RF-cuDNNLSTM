{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We input the model with 240 timesteps and 3 features and train it to predict the direction of the $241^{st}$ intraday return.\n",
    "\n",
    "More precisely, for each stock $s$ at time $t$, we first consider the following three features $ir^{(s)}_{t, 1}, cr^{(s)}_{t, 1}, or^{(s)}_{t, 1}$ defined above.\n",
    "\n",
    "Then we apply the Robust Scaler Standardization\n",
    "\n",
    "$\\tilde f^{(s)}_{t, 1} := \\dfrac {f^{(s)}_{t,1} - Q_2(f^{(s)}_{.,1})} {Q_3(f^{(s)}_{.,1}) - Q_1(f^{(s)}_{.,1})}$\n",
    "\n",
    "where $Q_1(f^{(s)}_{.,1}), Q_2(f^{(s)}_{.,1})$ and $Q_3(f^{(s)}_{.,1})$ are the first, second and third quartile of $f^{(s)}_{.,1}$, for each feature $f^{(s)}_{.,1} \\in \\{ ir^{(s)}_{., 1}, cr^{(s)}_{., 1}, or^{(s)}_{., 1} \\}$ in the respective training period.\n",
    "\n",
    "The Robust Scaler Standardization first subtracts (and hence removes) the median and then scales the data using the inter-quartile range, making it robust to outliers.\n",
    "\n",
    "Next for each time $t \\in \\{ 240, 241, ..., T_{study} \\}$, we generate overlapping sequence of 240 consecutive, three-dimensional standardized features $\\{ \\tilde F^{(s)}_{t-239,1}, \\tilde F^{(s)}_{t-238,1}, ..., \\tilde F^{(s)}_{t,1} \\}$, where $\\tilde F^{(s)}_{t-i,1} := (\\tilde ir^{(s)}_{t-i,1}, \\tilde cr^{(s)}_{t-i,1}, \\tilde or^{(s)}_{t-i,1}), i \\in \\{ 239, 238, ..., 0 \\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_scaler(f_t, f):\n",
    "    quartile_25 = np.percentile(f, 25, interpolation=\"midpoint\")\n",
    "    quartile_50 = np.median(f)\n",
    "    quartile_75 = np.percentile(f, 75, interpolation=\"midpoint\")\n",
    "    \n",
    "    inter_qrt_range = quartile_75 - quartile_25\n",
    "    \n",
    "    return (f - quartile_50) / inter_qrt_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DBZJPg65wYKT",
    "outputId": "112c68e8-df8c-4ae2-807b-d17d8508aff2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DEMO CODE üëá\n",
    "def func():\n",
    "    curr_dataset = main_datasets[0]\n",
    "    \n",
    "    # Take the total amount of days in 1st study period\n",
    "    T_study = curr_dataset.shape[1]\n",
    "\n",
    "    # Create the t =[241, 243, ..., T_study]\n",
    "    t = np.arange(240, T_study)\n",
    "    I = np.arange(239, -1, -1)\n",
    "    \n",
    "    # Define number of stocks as it will be used to create arrays with proper shapes\n",
    "    n_stocks = 251\n",
    "\n",
    "    # We use 240 consecutive 3 dimensional vectors to predict 241st return.\n",
    "    # Create a container to store ir, cr and or for the current dataset. Shape (251, 1008, 240, 3). F -> (240, 3)\n",
    "    container = np.ones(shape=(n_stocks, T_study, t[0], 3))\n",
    "\n",
    "    # ir_t-i,1 |  cr_t-i,1 | or_t-i,1\n",
    "    \n",
    "    \n",
    "\n",
    "    return container.shape\n",
    "\n",
    "\n",
    "func()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "25AbDpT-SQCk"
   },
   "source": [
    "#  Forecasting directional movements of stock prices for intraday trading using Random Forest & CuDNNLSTM\n",
    "\n",
    "-----------------------------------------------------------\n",
    "*What Is Intraday Return?*\n",
    "\n",
    "The intraday return is one of the two components of the total daily return generated by a stock. Intraday return measures the return generated by a stock during regular trading hours, based on its price change from the opening of a trading day to its close. Intraday return and overnight return together constitute the total daily return from a stock, which is based on the price change of a stock from the close of one trading day to the close of the next trading day. It is also called daytime return.\n",
    "\n",
    "Intraday return is of particular importance for day traders, who use daytime gyrations in stocks and markets to make trading profits, and rarely leave positions open overnight. Day trading strategies are not as commonplace for regular investors as they were before the 2008-2009 recession.\n",
    "\n",
    "---------------------------------------------------------\n",
    "*How to Calculate Daily Returns?*\n",
    "\n",
    "To calculate a daily return, you subtract the starting price from the closing price. Once you have that, you simply multiply by the number of shares you own.\n",
    "\n",
    "To illustrate, let's say you own `100` shares of XYZ stock. The day opens at `$20` and closes at `$25`. This is a `$5` positive difference. Multiply the `$5` difference by the `100` shares you own, for a daily return of `$500`.\n",
    "\n",
    "Some investors will prefer to work in percentages rather than dollar amounts. This is only slightly more complicated. You perform the same first step and arrive at a `$5` gain per share for the day. You then divide by the opening price of `$25`, leaving you with `0.2`. Multiply by `100` to arrive at your daily return of `20%`.\n",
    "\n",
    "-----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A7HVMz79RJ8v"
   },
   "source": [
    "This project is done by following the methods and techniques of the paper `Forecasting directional movements of stock prices for intraday trading using LSTM and random forests`. Link to the paper: [Click Here](https://arxiv.org/pdf/2004.10178.pdf).\n",
    "\n",
    "This introduces multi-feature setting consisting not only of the returns with respect to the closing prices, but also with respect to the opening prices and intraday returns to predict each stock, at the beginning of each day, the probablity to outperform the market in terms of intraday returns.\n",
    "\n",
    "As dataset we use all stocks of the S&P 500 from the period of January 1990 until December 2018.\n",
    "\n",
    "We employ both Random Forests on the one hand and LSTM on the other hand as training methodology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UyxAjLk6cXMc"
   },
   "source": [
    "### Technology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VOFu4PhEdkK3"
   },
   "source": [
    "- Python: 3.9.16\n",
    "- Scikit-Learn: 1.2.2\n",
    "- Tensorflow: 2.12.0\n",
    "- System RAM: 12.7 GB\n",
    "- GPU RAM: 15.0 GB\n",
    "- Disk: 78.2 GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7JsDmuufYS1K"
   },
   "source": [
    "## Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jErs_PBbSjM9"
   },
   "outputs": [],
   "source": [
    "# For data processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load pickle file\n",
    "import pickle\n",
    "\n",
    "# For Random Forest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# LSTM and other layers\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EmhPxpT6t8HK"
   },
   "source": [
    "## Download the stocks and save it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to `Collect Stock Data and Store it.ipynb` file to download the stocks data and save it.\n",
    "\n",
    "Then come back to this file. But if you have already downloaded the data by running that file once then no need to run it again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0GbRIIBKfUGT"
   },
   "source": [
    "## Steps to follow - \n",
    "1. We divide our raw data into study periods, where each study period is divided into a training part(for in-sample trading) and a trading part(for out-sample predictions).\n",
    "2. We introduce out features.\n",
    "3. We set up our targets.\n",
    "4. We define our 2 machine learning methods we employ, namely random forest and CuDNNLSTM.\n",
    "5. Establish a trading strategy for trading part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gsitj69EeR0U"
   },
   "source": [
    "## Data preparation for Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBHNC--xEvo5"
   },
   "source": [
    "\n",
    "### Preparing the original dataset for further processing\n",
    "\n",
    "*Segregate the stocks within different numpy arrays according to the ticker name.*\n",
    "![Stack-stock-data-on-top-of-each-other.png](https://i.postimg.cc/HnDyJp9s/Stack-stock-data-on-top-of-each-other.png)\n",
    "\n",
    "WARNING‚ùó \n",
    "- Different stocks will give different no. of rows, as all stocks were not always available in that time span.\n",
    "- Here we are not able to properly get all days values.\n",
    "\n",
    "To prevent the error, we are removing those stocks will are having empty values for those dates.\n",
    "\n",
    "E.g. Stock A might start from 1991-01-01 and Stock B from 1990-12-31 then both of them are not in same shape. So we can remove stock B. Making sure all stocks are having same no of rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9luoTa3nnVa8"
   },
   "source": [
    "### Datasets creation with non-overlapping testing period from original dataset\n",
    "\n",
    "We divide the dataset contsisting of 29 years starting from January 1990 till December 2018, using a 4-year window, 1-year stride, where each study period is divided into a training part(of 756 days almost = 3 years) and trading part(of 252 days almost = 1 year).\n",
    "\n",
    "So, we obtain 26 study periods with non-overlapping trading part.\n",
    "\n",
    "![Dataset-creation-with-non-overlapping-testing-period.png](https://i.postimg.cc/7YWw5YwL/Dataset-creation-with-non-overlapping-testing-period.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BNa54mX2juGf"
   },
   "source": [
    "Refer to `Generating List of Datasets and Saving it.ipynb` file for the steps of `Preparing the original dataset for further processing` and `Datasets creation with non-overlapping testing period from original dataset`.\n",
    "\n",
    "After running all the cells, come back this file. Now you are ready to run the below cells as it is loading the `datasets-list` pickle file saved by `Generating List of Datasets and Saving it.ipynb` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcJwB422SGMb"
   },
   "source": [
    "## Features Selection\n",
    "\n",
    "Let $T_{study}$ denote the total amount of days in a study period and $n_i$ represent the number of stocks $s$ in $S$ having complete historical data available at the end of each study period $i$. Moreover, we define the adjacent closing price and opening price of any stock $s \\in S$ at time $t$ by $cp^{(s)}_t$ and $op^{(s)}_t$.\n",
    "\n",
    "Given a prediction day $t:=\\tau$, we have the following inputs and prediction task.\n",
    "\n",
    "Input: We have the historical opening prices, $op^{(s)}_t, t \\in \\{ 0, 1, ..., \\tau -1, \\tau\\}$, (including the opening price of the prediction day $op^{(s)}_\\tau$) as well as the historical adjacent closing prices, $cp^{(s)}_t, t \\in \\{ 0, 1, ..., \\tau -1\\}$, (excluding the opening price of the prediction day $cp^{(s)}_\\tau$).\n",
    "\n",
    "Task: Out of all n stocks, predict k stocks with highest and k stocks with lowest intraday return $ir_{\\tau, 0} = \\dfrac{cp_\\tau}{op_\\tau} - 1$.\n",
    "\n",
    "**NOTE:** In the original paper they used all the stocks that could be scrapped from the web. Then they divided each stock into 26 datasets. Now, in this 26 datasets, some datasets may contain all 492 stocks that were originally scrapped and some datasets may contain only 251 stocks. That is why it is saying $s \\in S$ because each dataset will have different number of stocks and that will be a subset of all the originally scrapped stocks.\n",
    "\n",
    "But, in our case we are dealing with only those stocks which has all entries filled from 1990-01-02 to 2018-12-31. So, we have 251 stocks in all the datasets.\n",
    "\n",
    "\n",
    "\n",
    "For LaTex markdown, refer to this page: [here](https://ashki23.github.io/markdown-latex.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dus29JN9SJUZ"
   },
   "source": [
    "### Feature generation for Random Forest\n",
    "\n",
    "For any stock $s \\in S$ and any time $t \\in \\{ 241, 242, ..., T_{study} \\}$, the feature set we provide to the random forest comprises of 3 signal:\n",
    "\n",
    "1. Intraday return: $ir^{(s)}_{t, m} := \\dfrac{cp^{(s)}_{t-m}}{op^{(s)}_{t-m}} - 1$,\n",
    "\n",
    "\n",
    "2. Returns with respect to last closing price: $cr^{(s)}_{t, m} := \\dfrac{cp^{(s)}_{t-1}}{cp^{(s)}_{t-1-m}} - 1$,\n",
    "\n",
    "\n",
    "3. Returns with respect to opening price: $or^{(s)}_{t, m} := \\dfrac{op^{(s)}_{t}}{cp^{(s)}_{t-m}} - 1$,\n",
    "\n",
    "where $m \\in \\{ 1, 2, 3, ..., 20 \\} \\cup \\{ 40, 60, 80, ...., 240 \\}$, obtaining 93 features. By the choice of m we consider in the first month the corresponding returns of each trading day, whereas for the subsequent 11 months we only consider the corresponding multi-period returns of each month.\n",
    "\n",
    "![ir-cr-and-or-calculation.png](https://i.postimg.cc/T3sjRDQ1/ir-cr-and-or-calculation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_ir(cp: np.ndarray, op: np.ndarray, t: int, m: int) -> np.float64:\n",
    "    '''\n",
    "    Calculates Intraday return of t,m for one t and one m\n",
    "    \n",
    "    args:\n",
    "        cp -> 1D array of closing prices for stock s\n",
    "        op -> 1D array of opening prices for stock s\n",
    "        t -> time at which we want the intraday return\n",
    "        m -> How much we will subtract from t\n",
    "        \n",
    "    returns:\n",
    "        Intraday return of t-th day\n",
    "    '''\n",
    "    \n",
    "    return cp[t-m] / op[t-m] - 1\n",
    "\n",
    "def calc_cr(cp: np.ndarray, t: int, m: int) -> np.float64:\n",
    "    '''\n",
    "    Calculates return with respect to last closing price for t, m\n",
    "    \n",
    "    args:\n",
    "        cp -> 1D array of closing prices for stock s\n",
    "        t -> time at which we want the intraday return\n",
    "        m -> how many days before the t-th day\n",
    "        \n",
    "    returns:\n",
    "        Return with respect to last closing price on t-th day\n",
    "    '''\n",
    "    \n",
    "    return cp[t-1] / cp[t-1-m] - 1\n",
    "\n",
    "def calc_or(cp: np.ndarray, op: np.ndarray, t: int, m: int) -> np.float64:\n",
    "    '''\n",
    "    Calculates return with respect to opening price for t, m\n",
    "    \n",
    "    args:\n",
    "        cp -> 1D array of closing prices for stock s\n",
    "        op -> 1D array of opening prices for stock s\n",
    "        t -> time at which we want the intraday return\n",
    "        m -> how many days before the t-th day\n",
    "        \n",
    "    returns:\n",
    "        Return with respect to last closing price on t-th day\n",
    "    '''\n",
    "    \n",
    "    return op[t] / cp[t-m] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "3EuH1cKcGM0j"
   },
   "outputs": [],
   "source": [
    "# This function will generate new features for Random Forest\n",
    "def generate_features_rf(curr_dataset):\n",
    "\n",
    "    # Take the total amount of days in 1st study period\n",
    "    T_study = curr_dataset.shape[1]\n",
    "    print(\"current dataset has\", T_study, \" days.\")\n",
    "\n",
    "    # Create the t =[241, 243, ..., T_study]\n",
    "    t = np.arange(240, T_study)\n",
    "\n",
    "    # Define the m for calculation of t-m, m = [1, 2, 3, ..., 20]\n",
    "    M = np.arange(1, 21)\n",
    "\n",
    "    # m = [1, 2, 3, ..., 20] U [40, 60, 80, ..., 240]\n",
    "    M = np.concatenate((M, np.arange(40, 241, 20)))\n",
    "\n",
    "    # Define number of stocks as it will be used to create arrays with proper shapes\n",
    "    n_stocks = 251\n",
    "\n",
    "    # Create a container to store ir, cr and or for the current dataset\n",
    "    container = np.ones(shape=(n_stocks, T_study, M.shape[0]*3))\n",
    "\n",
    "    # Put NaN values to the first 240 rows as it will be used for feature creation\n",
    "    container[:, :t[0], :] = np.nan \n",
    "\n",
    "\n",
    "    # To calculate ir, we need cp_(t-m) and op_(t-m)\n",
    "    cp_t_m = np.zeros((n_stocks, t.shape[0], M.shape[0]))\n",
    "    op_t_m = np.zeros((n_stocks, t.shape[0], M.shape[0]))\n",
    "\n",
    "    # To calculate cr, we need cp_(t-1-m) and cp_(t-1-m). Remember we are indexing from 0, not 1!\n",
    "    cp_t_1_m = np.zeros((n_stocks, t.shape[0], M.shape[0]))\n",
    "    cp_t_1 = curr_dataset[:, t-2, 0]\n",
    "\n",
    "    # To calculate or, we need op_t and cp_t_m. Remember we are indexing from 0, not 1!\n",
    "    op_t = curr_dataset[:, t-1, 1]\n",
    "\n",
    "\n",
    "    # Calculate cp_(t-m), op_(t-m) and cp_(t-1-m) for each m and store them at proper axis=2 index i\n",
    "    # of their respective container\n",
    "    for i, m in enumerate(M):\n",
    "        cp_t_m[:, :, i] = curr_dataset[:, t-m, 0]\n",
    "        op_t_m[:, :, i] = curr_dataset[:, t-m, 1]\n",
    "        cp_t_1_m[:, :, i] = curr_dataset[:, t-1-m, 0]\n",
    "\n",
    "\n",
    "    # Calculate ir_(t-m)\n",
    "    ir_t_m = np.divide(cp_t_m, op_t_m, out=np.zeros_like(cp_t_m), where=op_t_m!=0) - 1\n",
    "\n",
    "\n",
    "    # Before calculating cr_(t-m), reshape the cp_(t-1-m) as it should have the same last part of shape as cp_(t-1), the divident\n",
    "    # means if cp_(t-1) is (251, 774) then cp_(t-1-m) should be (_, 251, 774) notice the last of shape is same\n",
    "    reshaped_cp_t_1_m = cp_t_1_m.reshape(M.shape[0], n_stocks, -1)\n",
    "\n",
    "    # Calculating cr_(t-m)\n",
    "    cr_t_m = np.divide(cp_t_1, reshaped_cp_t_1_m, where=reshaped_cp_t_1_m!=0).reshape(n_stocks, -1, M.shape[0]) - 1\n",
    "\n",
    "\n",
    "    # Before calculating or_(t-m), reshape the cp_(t-m) as it should have the same last part of shape as op_t, the divident\n",
    "    # means if op_t is (251, 774) then cp_(t-m) should be (_, 251, 774) notice the last of shape is same\n",
    "    reshaped_cp_t_m = cp_t_m.reshape(M.shape[0], n_stocks, -1)\n",
    "\n",
    "    # Calculating or_(t-m)\n",
    "    or_t_m = np.divide(op_t, reshaped_cp_t_m, where=reshaped_cp_t_m!=0).reshape(n_stocks, -1, M.shape[0]) - 1\n",
    "\n",
    "\n",
    "    # Put the ir, cr and or inside the container\n",
    "    container[:, t, :] = np.dstack((ir_t_m, cr_t_m, or_t_m))\n",
    "\n",
    "    return container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LGRA1qIdxSDH",
    "outputId": "5273cfe5-6dc8-454e-f7f6-5e2b4178849a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current dataset has 1013  days.\n",
      "current dataset has 1012  days.\n",
      "current dataset has 1011  days.\n",
      "current dataset has 1011  days.\n",
      "current dataset has 1011  days.\n",
      "current dataset has 1011  days.\n",
      "current dataset has 1011  days.\n",
      "current dataset has 1009  days.\n",
      "current dataset has 1004  days.\n",
      "current dataset has 1004  days.\n",
      "current dataset has 1004  days.\n",
      "current dataset has 1004  days.\n",
      "current dataset has 1008  days.\n",
      "current dataset has 1007  days.\n",
      "current dataset has 1006  days.\n",
      "current dataset has 1007  days.\n",
      "current dataset has 1007  days.\n",
      "current dataset has 1008  days.\n",
      "current dataset has 1009  days.\n",
      "current dataset has 1006  days.\n",
      "current dataset has 1006  days.\n",
      "current dataset has 1006  days.\n",
      "current dataset has 1006  days.\n",
      "current dataset has 1008  days.\n",
      "current dataset has 1007  days.\n",
      "current dataset has 1005  days.\n"
     ]
    }
   ],
   "source": [
    "# It will contain all the newly processed datasets each with a shape (251, stock days in 4 years, 93)\n",
    "containers = []\n",
    "\n",
    "# Run the generate_feature_rf function for each dataset inside main_datasets\n",
    "for dataset in main_datasets:\n",
    "    containers.append(generate_features_rf(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bmfSm43DyHD1",
    "outputId": "53dd852b-b649-41d0-f3aa-9547a88315ea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(containers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PvuESLpv1-nq"
   },
   "source": [
    "### Feature generation for LSTM\n",
    "\n",
    "We input the model with 240 timesteps and 3 features and train it to predict the direction of the $241^{st}$ intraday return.\n",
    "\n",
    "More precisely, for each stock $s$ at time $t$, we first consider the following three features $ir^{(s)}_{t, 1}, cr^{(s)}_{t, 1}, or^{(s)}_{t, 1}$ defined above.\n",
    "\n",
    "Then we apply the Robust Scaler Standardization\n",
    "\n",
    "$\\tilde f^{(s)}_{t, 1} := \\dfrac {f^{(s)}_{t,1} - Q_2(f^{(s)}_{.,1})} {Q_3(f^{(s)}_{.,1}) - Q_1(f^{(s)}_{.,1})}$\n",
    "\n",
    "where $Q_1(f^{(s)}_{.,1}), Q_2(f^{(s)}_{.,1})$ and $Q_3(f^{(s)}_{.,1})$ are the first, second and third quartile of $f^{(s)}_{.,1}$, for each feature $f^{(s)}_{.,1} \\in \\{ ir^{(s)}_{., 1}, cr^{(s)}_{., 1}, or^{(s)}_{., 1} \\}$ in the respective training period.\n",
    "\n",
    "The Robust Scaler Standardization first subtracts (and hence removes) the median and then scales the data using the inter-quartile range, making it robust to outliers.\n",
    "\n",
    "Next for each time $t \\in \\{ 240, 241, ..., T_study \\}$, we generate overlapping sequence of 240 consecutive, three-dimensional standardized features $\\{ \\tilde F^{(s)}_{t-239,1}, \\tilde F^{(s)}_{t-238,1}, ..., \\tilde F^{(s)}_{t,1} \\}$, where $\\tilde F^{(s)}_{t-i,1} := (\\tilde ir^{(s)}_{t-i,1}, \\tilde cr^{(s)}_{t-i,1}, \\tilde or^{(s)}_{t-i,1}), i \\in \\{ 239, 238, ..., 0 \\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rg1yPnfL2EAP"
   },
   "outputs": [],
   "source": [
    "def generate_features_lstm(curr_dataset):\n",
    "    '''\n",
    "    This function generates features for LSTM.\n",
    "    '''\n",
    "    \n",
    "    m = 1\n",
    "    t = np.arange(1, 21)\n",
    "    t = np.concatenate(t, np.arange(40, 241, 20))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "7JsDmuufYS1K",
    "Gzrx5Q7DYixY",
    "oBHNC--xEvo5",
    "9luoTa3nnVa8",
    "dus29JN9SJUZ"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "stock-kernel",
   "language": "python",
   "name": "stock-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
